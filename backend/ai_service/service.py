"""
Servicio de integraci√≥n con DeepSeek v√≠a Ollama.
Genera explicaciones cient√≠ficas de reacciones qu√≠micas.
"""

import requests
import logging
import json
from django.conf import settings

logger = logging.getLogger(__name__)


class DeepSeekService:
    """
    Cliente para comunicaci√≥n con Ollama ejecutando modelos de IA.
    
    IMPORTANTE: Este servicio NUNCA inventa reacciones.
    Solo explica reacciones ya validadas en la base de datos.
    """
    
    # Cache para evitar verificar disponibilidad en cada llamada
    _availability_cache = None
    _availability_cache_time = None
    
    def __init__(self):
        self.base_url = getattr(settings, 'OLLAMA_BASE_URL', 'http://localhost:11434')
        # Usar llama3.2 como modelo por defecto
        self.model = getattr(settings, 'OLLAMA_MODEL', 'llama3.2:latest')
        self.timeout = 120  # Timeout razonable para respuestas
    
    def is_available(self, use_cache=True):
        """Verifica si Ollama est√° disponible (con cache de 60 segundos)."""
        import time
        
        # Usar cache si est√° disponible y no ha expirado (60 segundos)
        if use_cache and DeepSeekService._availability_cache is not None:
            if time.time() - DeepSeekService._availability_cache_time < 60:
                return DeepSeekService._availability_cache
        
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=3)
            available = response.status_code == 200
            if available:
                models = response.json().get('models', [])
                logger.info(f"Ollama disponible. Modelos: {[m.get('name') for m in models]}")
            
            # Guardar en cache
            DeepSeekService._availability_cache = available
            DeepSeekService._availability_cache_time = time.time()
            return available
        except requests.RequestException as e:
            logger.error(f"Ollama no disponible: {e}")
            DeepSeekService._availability_cache = False
            DeepSeekService._availability_cache_time = time.time()
            return False
    
    def get_available_models(self):
        """Obtiene lista de modelos disponibles en Ollama."""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                return [m.get('name') for m in response.json().get('models', [])]
            return []
        except requests.RequestException:
            return []
    
    def explain_element(self, element, level='intermediate'):
        """
        Genera explicaci√≥n cient√≠fica detallada de un elemento qu√≠mico.
        
        Args:
            element: Objeto Element de la base de datos
            level: 'basic', 'intermediate', o 'advanced'
        
        Returns:
            str: Explicaci√≥n generada por el modelo de IA
        """
        prompt = self._build_element_prompt(element, level)
        
        try:
            if not self.is_available():
                logger.warning("Ollama no disponible para explicaci√≥n de elemento")
                return self._get_fallback_element_explanation(element, level)
            
            # Tokens adaptativos por nivel
            token_limits = {'basic': 300, 'intermediate': 500, 'advanced': 800}
            max_tokens = token_limits.get(level, 500)
            
            response = self._call_ollama(prompt, max_tokens=max_tokens)
            cleaned = self._clean_response(response)
            
            if not cleaned or len(cleaned.strip()) < 20:
                logger.warning(f"Respuesta vac√≠a para elemento: {element.symbol}")
                return self._get_fallback_element_explanation(element, level)
            
            return cleaned
            
        except Exception as e:
            logger.error(f"Error explicando elemento {element.symbol}: {e}")
            return self._get_fallback_element_explanation(element, level)
    
    def _build_element_prompt(self, element, level):
        """Construye prompt para explicar un elemento."""
        
        level_instructions = {
            'basic': "Explica para un estudiante de secundaria. Usa lenguaje simple. M√°ximo 150 palabras.",
            'intermediate': "Explica para un estudiante universitario. Incluye aplicaciones y propiedades importantes. M√°ximo 250 palabras.",
            'advanced': "Explica a nivel profesional. Incluye configuraci√≥n electr√≥nica detallada, propiedades qu√≠micas avanzadas y aplicaciones industriales. M√°ximo 400 palabras."
        }
        
        return f"""Eres un profesor de qu√≠mica experto. Explica el siguiente elemento qu√≠mico en espa√±ol:

Elemento: {element.name} ({element.symbol})
N√∫mero at√≥mico: {element.atomic_number}
Masa at√≥mica: {element.atomic_mass} u
Categor√≠a: {element.category}
Configuraci√≥n electr√≥nica: {element.electron_config}
Electrones de valencia: {element.valence_electrons}
Electronegatividad: {element.electronegativity or 'N/A'}
Per√≠odo: {element.period}, Grupo: {element.group}

{level_instructions.get(level, level_instructions['intermediate'])}

Incluye:
1. Propiedades f√≠sicas y qu√≠micas principales
2. D√≥nde se encuentra en la naturaleza
3. Usos y aplicaciones importantes
4. Datos curiosos o hist√≥ricos

Responde SOLO con la explicaci√≥n, sin introducci√≥n ni despedida."""
    
    def _get_fallback_element_explanation(self, element, level):
        """Genera explicaci√≥n b√°sica cuando la IA no est√° disponible."""
        
        category_names = {
            'alkali-metal': 'metal alcalino',
            'alkaline-earth': 'metal alcalinot√©rreo',
            'transition-metal': 'metal de transici√≥n',
            'post-transition-metal': 'metal post-transici√≥n',
            'metalloid': 'metaloide',
            'nonmetal': 'no metal',
            'halogen': 'hal√≥geno',
            'noble-gas': 'gas noble',
            'lanthanide': 'lant√°nido',
            'actinide': 'act√≠nido'
        }
        
        category_name = category_names.get(element.category, element.category)
        
        return f"""{element.name} ({element.symbol}) es un elemento qu√≠mico clasificado como {category_name}.

**Propiedades b√°sicas:**
‚Ä¢ N√∫mero at√≥mico: {element.atomic_number}
‚Ä¢ Masa at√≥mica: {element.atomic_mass:.4f} u
‚Ä¢ Configuraci√≥n electr√≥nica: {element.electron_config}
‚Ä¢ Electrones de valencia: {element.valence_electrons}

**Ubicaci√≥n en la tabla peri√≥dica:**
Se encuentra en el per√≠odo {element.period} y grupo {element.group}.

**Propiedades f√≠sicas:**
‚Ä¢ Electronegatividad: {element.electronegativity or 'No disponible'}
‚Ä¢ Punto de fusi√≥n: {element.melting_point or 'No disponible'} ¬∞C
‚Ä¢ Punto de ebullici√≥n: {element.boiling_point or 'No disponible'} ¬∞C

Este elemento tiene {element.atomic_number} protones en su n√∫cleo y t√≠picamente {element.atomic_number} electrones en su configuraci√≥n neutra."""

    def explain_reaction(self, reaction, level='intermediate'):
        """
        Genera explicaci√≥n cient√≠fica de una reacci√≥n qu√≠mica.
        
        Args:
            reaction: Objeto Reaction de la base de datos
            level: 'basic', 'intermediate', o 'advanced'
        
        Returns:
            str: Explicaci√≥n generada por el modelo de IA
        """
        prompt = self._build_prompt(reaction, level)
        
        try:
            # Verificar disponibilidad primero
            if not self.is_available():
                logger.warning("Ollama no est√° disponible, usando fallback")
                return self._get_fallback_explanation(reaction, level)
            
            # Tokens adaptativos por nivel para reacciones (optimizado para velocidad)
            token_limits = {'basic': 200, 'intermediate': 350, 'advanced': 500}
            max_tokens = token_limits.get(level, 350)
            
            # Intentar obtener respuesta
            response = self._call_ollama(prompt, max_tokens=max_tokens)
            cleaned = self._clean_response(response)
            
            # Validar que la respuesta no est√© vac√≠a
            if not cleaned or len(cleaned.strip()) < 20:
                logger.warning(f"Respuesta vac√≠a o muy corta del modelo: '{response[:100] if response else 'None'}'")
                return self._get_fallback_explanation(reaction, level)
            
            return cleaned
            
        except Exception as e:
            logger.error(f"Error calling AI model: {e}", exc_info=True)
            # Fallback a descripci√≥n almacenada
            return reaction.description or self._get_fallback_explanation(reaction, level)
    
    def _build_prompt(self, reaction, level):
        """Construye el prompt para DeepSeek."""
        
        level_instructions = {
            'basic': """
Explica esta reacci√≥n qu√≠mica para un estudiante de secundaria.
Usa lenguaje simple y ejemplos cotidianos.
Evita t√©rminos t√©cnicos complejos.
M√≠nimo 200 palabras, m√°ximo 300 palabras.

Incluye:
1. Qu√© sucede paso a paso en la reacci√≥n
2. Por qu√© ocurre esta reacci√≥n
3. Un ejemplo de la vida cotidiana donde se ve esto
4. Qu√© observar√≠amos si hici√©ramos esta reacci√≥n
""",
            'intermediate': """
Explica esta reacci√≥n qu√≠mica para un estudiante universitario de primer a√±o.
M√≠nimo 400 palabras, m√°ximo 500 palabras.

Incluye:
1. Descripci√≥n detallada del mecanismo de reacci√≥n
2. Tipos de enlaces que se rompen y se forman
3. An√°lisis de electronegatividad de los elementos involucrados
4. Explicaci√≥n energ√©tica (por qu√© es exot√©rmica o endot√©rmica)
5. Condiciones necesarias para que ocurra
6. Aplicaciones pr√°cticas en la industria o laboratorio
7. Precauciones de seguridad relevantes
""",
            'advanced': """
Explica esta reacci√≥n qu√≠mica con rigor cient√≠fico avanzado y profesional.
M√≠nimo 600 palabras, m√°ximo 800 palabras.

Incluye:
1. Mecanismo de reacci√≥n detallado paso a paso
2. Teor√≠a de orbitales moleculares involucrados
3. An√°lisis termodin√°mico completo (ŒîH, ŒîG, ŒîS)
4. Cin√©tica de la reacci√≥n y factores que la afectan
5. Estados de transici√≥n y energ√≠a de activaci√≥n
6. Configuraciones electr√≥nicas de reactivos y productos
7. Aplicaciones industriales y de investigaci√≥n
8. Historia del descubrimiento de esta reacci√≥n
9. Variantes y reacciones relacionadas
10. Impacto ambiental o tecnol√≥gico si es relevante
"""
        }
        
        # Construir contexto de la reacci√≥n
        reaction_context = f"""
REACCI√ìN: {reaction.equation}
TIPO: {reaction.get_reaction_type_display()}
REACTIVOS: {', '.join([r.get('symbol', r.get('formula', '')) for r in reaction.reactants])}
PRODUCTOS: {', '.join([p.get('formula', p.get('name', '')) for p in reaction.products])}
CAMBIO DE ENTALP√çA: {reaction.enthalpy_change or 'No especificado'} kJ/mol
REACCI√ìN {'EXOT√âRMICA' if reaction.is_exothermic else 'ENDOT√âRMICA'}
"""
        
        prompt = f"""Eres un profesor de qu√≠mica experto. Tu tarea es explicar la siguiente reacci√≥n qu√≠mica REAL.

{reaction_context}

INSTRUCCIONES:
{level_instructions.get(level, level_instructions['intermediate'])}

IMPORTANTE:
- Solo explica lo que REALMENTE ocurre en esta reacci√≥n
- No inventes informaci√≥n ni reacciones alternativas
- S√© preciso y educativo
- Responde en espa√±ol

Tu explicaci√≥n:"""
        
        return prompt
    
    def _call_ollama(self, prompt, max_tokens=None):
        """Llama a la API de Ollama con par√°metros optimizados."""
        url = f"{self.base_url}/api/generate"
        
        # Tokens adaptativos basados en nivel: optimizado para velocidad
        num_tokens = max_tokens or 400  # Reducido para respuestas m√°s r√°pidas
        
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": True,  # Activar streaming para ver progreso
            "options": {
                "temperature": 0.7,  # Balance entre creatividad y coherencia
                "top_p": 0.9,
                "num_predict": num_tokens,
                "num_ctx": 2048  # Contexto reducido para mayor velocidad
            }
        }
        
        logger.info(f"üöÄ Llamando a Ollama API con modelo: {self.model}")
        logger.info(f"üìù Tokens m√°ximos: {num_tokens}")
        
        full_response = ""
        token_count = 0
        
        try:
            # Usar streaming para ver progreso
            with requests.post(url, json=payload, timeout=self.timeout, stream=True) as response:
                response.raise_for_status()
                
                for line in response.iter_lines():
                    if line:
                        try:
                            data = json.loads(line)
                            token = data.get('response', '')
                            full_response += token
                            token_count += 1
                            
                            # Log cada 20 tokens para ver progreso
                            if token_count % 20 == 0:
                                logger.info(f"‚è≥ Generando... {token_count} tokens ({len(full_response)} chars)")
                            
                            # Verificar si termin√≥
                            if data.get('done', False):
                                logger.info(f"‚úÖ Generaci√≥n completada: {token_count} tokens, {len(full_response)} chars")
                                break
                        except json.JSONDecodeError:
                            continue
            
            logger.info(f"üìä Respuesta final: {len(full_response)} caracteres")
            
            if not full_response:
                logger.warning(f"‚ö†Ô∏è Respuesta vac√≠a de Ollama")
            else:
                # Mostrar primeros 200 chars en log
                logger.info(f"üìÑ Preview: {full_response[:200]}...")
            
            return full_response
            
        except requests.exceptions.Timeout:
            logger.error(f"‚è±Ô∏è Timeout despu√©s de {self.timeout}s")
            raise
        except Exception as e:
            logger.error(f"‚ùå Error en Ollama: {e}")
            raise
    
    def _clean_response(self, response):
        """Limpia la respuesta del modelo de IA."""
        if not response:
            return ""
            
        import re
        
        # Remover etiquetas <think>...</think> si existen (usadas por DeepSeek-R1)
        response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)
        
        # Remover otros posibles patrones de "pensamiento"
        response = re.sub(r'\[THINKING\].*?\[/THINKING\]', '', response, flags=re.DOTALL | re.IGNORECASE)
        response = re.sub(r'\*\*Pensando\*\*:.*?(?=\n\n|\Z)', '', response, flags=re.DOTALL)
        
        # Remover prefijos comunes que el modelo podr√≠a agregar
        response = re.sub(r'^(Explicaci√≥n:|Tu explicaci√≥n:|Respuesta:)\s*', '', response, flags=re.IGNORECASE)
        
        # Normalizar espacios en blanco
        response = re.sub(r'\n{3,}', '\n\n', response)  # M√°ximo 2 saltos de l√≠nea consecutivos
        response = re.sub(r' {2,}', ' ', response)  # Un solo espacio
        
        return response.strip()
    
    def _get_fallback_explanation(self, reaction, level):
        """Genera explicaci√≥n de respaldo cuando Ollama no est√° disponible."""
        basic = f"""
Esta es una reacci√≥n de {reaction.get_reaction_type_display().lower()}.
En ella, {self._describe_reactants(reaction)} se combinan para formar {self._describe_products(reaction)}.
{'Esta reacci√≥n libera energ√≠a (exot√©rmica).' if reaction.is_exothermic else 'Esta reacci√≥n absorbe energ√≠a (endot√©rmica).'}
"""
        
        if level == 'basic':
            return basic.strip()
        
        intermediate = basic + f"""
El cambio de entalp√≠a es de {reaction.enthalpy_change or 'un valor no especificado'} kJ/mol.
"""
        
        if level == 'intermediate':
            return intermediate.strip()
        
        # Advanced incluye aplicaciones
        apps = reaction.real_world_applications
        apps_text = f"\n\nAplicaciones: {', '.join(apps)}" if apps else ""
        
        return (intermediate + apps_text).strip()
    
    def _describe_reactants(self, reaction):
        """Describe los reactivos en lenguaje natural."""
        names = []
        for r in reaction.reactants:
            if 'symbol' in r:
                names.append(r['symbol'])
            elif 'formula' in r:
                names.append(r['formula'])
        return ' y '.join(names) if names else 'los reactivos'
    
    def _describe_products(self, reaction):
        """Describe los productos en lenguaje natural."""
        names = []
        for p in reaction.products:
            if 'name' in p:
                names.append(p['name'])
            elif 'formula' in p:
                names.append(p['formula'])
        return ' y '.join(names) if names else 'los productos'
